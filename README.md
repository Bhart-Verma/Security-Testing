# 🤖 LLM Testing Guides

This repository contains a growing collection of practical, hands-on testing guides for identifying and understanding vulnerabilities in Large Language Models (LLMs), AI systems, and agent frameworks.

Whether you're a security researcher, penetration tester, or exploring the intersection of cybersecurity and AI, these guides cover areas such as:

- 🔓 Prompt Injection
- 🛡️ Output Manipulation & Jailbreak Bypasses
- 📦 API Fuzzing for LLM Services
- 🔄 Context Poisoning & Trust Exploits
- ⚙️ Tool Integration Risks in Agent Frameworks
- 🧠 MCP (Model Context Protocol) Testing

---

## 📚 Contents

> This list will grow as projects and test cases evolve.

- [MCP Security Testing Guide](./MCP-Security-Testing-Guide.md) – *Covers context-level vulnerabilities, tool shadowing, input tampering, and threat modeling in LLM-powered environments.*

---

## 🧠 Why This Repo?

LLMs are redefining software interfaces and expanding the attack surface. This repository helps document real-world, reproducible test cases to support AI security research and practical testing.

---

## 📄 License

This project is licensed under the MIT License. See [`LICENSE`](./LICENSE) for details.
